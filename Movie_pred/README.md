# NLP practice: 문장 감정 예측
## 0. 개요
bi-LSTM과 워드 임베딩 등을 활용하여 [네이버 영화 평점 데이터셋 ](https://github.com/e9t/nsmc/)을 가지고 문장 감정 예측을 할 수 있는 모델을 제작해보겠습니다. 이번 코드는 Google Colab(.ipynb)에서 작성되었습니다.

### 0-1. 목차
0. [개요](#0-개요)
1. [전처리](#1-전처리)
2. [딕셔너리 및 모델 구현](#2-딕셔너리-및-모델-구현)
3. [학습 및 테스트](#3-학습-및-테스트)

### 0-2. 파일 설명
*  data: 
	* ratings_train.txt, ratings_test.txt: 학습, 테스트 데이터
	* stopwords: 보편적으로 사용되는 [한국어 불용어 리스트](https://www.ranks.nl/stopwords/korean), 자주 등장하지만 큰 의미가 없는 단어들
	* test.txt, train.txt: 전처리가 끝난 train, test 데이터, pickle로 저장되어있음
* example: 모델이 예측한 결과 예시
* model: 용량 상의 문제로 업로드 x (168Mb)
* preprocessing.ipynb: 전처리
* movie_pred.ipynb: 모델 구현 및 학습, 테스트
### 0-3. 데이터셋 설명

[Naver sentiment movie corpus v1.0](https://github.com/e9t/nsmc/)
|id| document |label|
|--|--|--|
| 9976970 | 아 더빙.. 진짜 짜증나네요 목소리 |0|
|3819312|흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나|1|
|10265843|너무재밓었다그래서보는것을추천한다|0|
|9045019|교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정|0
|6483659|사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다|1|

위 표와 같이 데이터 셋이 구성되어있으며 train 데이터의 경우 15만개, test 데이터는 5만개로 구성되어있다. label의 경우 네이버 영화 리뷰 평점을 기준으로 **1~4**를 부정적인 문장, **9~10**을 긍정적인 문장, **5~8**의 중립적인 문장은 제외하여 구분하였습니다.

## 1. 전처리
*  정규식을 이용한 특수문자 제거
*  konlpy의 okt 클래스 이용 토크나이징
*  불용어 제외
*  결측치 혹은 빈문장 제거

## 2. 딕셔너리 및 모델 구현
### 2-1. 딕셔너리
전처리된 train 데이터의 딕셔너리 생성. 파이썬의 딕셔너리를 활용하여 단어에 대응되는 숫자, 숫자의 대응되는 단어를 반환해주며 0은 패딩, 1은 딕서너리에 없는 단어 처리로 정해주었습니다. (단어 수 약 43000)
### 2-2. 모델 구현
* pytorch의 nn.Embedding패키지 이용 임베딩 (임베딩 차원 300) nn.Embedding 을 이용할 때에는 LongTensor로 변환해주어야 합니다.
* bi-LSTM모델 활용 (히든레이어 차원 300, 레이어 수 1개, dropout = 0.1)
* pytorch의 pack_padded_sequence(), pad_packed_sequence()활용 (최대 길이 50) 패딩 제거
* fc 레이어 2층으로 구성
	* 1층 입력: 양방향이므로 300*2=600,  출력: 300
	* 2층 입력: 300, 출력: 2개를 classification해야하므로 2, sigmoid 통과

## 3. 학습 및 테스트
### 3-1. 학습
* loss function: cross entropy loss
* optimizer: Adam

총 30 epoch 학습 시켰으며, 데이터 셋이 단순하기 때문에 10 epoch정도만 되도 어느 정도 수렴하는 모습을 보였습니다.
### 3-2. 테스트
근소한 차이지만 30 epoch에서 가장 좋은 성능을 보여주었으며 84.61%의 정확성을 보였습니다. 그렇게 높은 성능은 나오지 않았습니다. 여러가지 이유가 있겠지만 네이버 리뷰라는 데이터셋의 출처 특성상 문법이 제대로 지켜져 있지 않고 비꼬는 말투도 많이 포함되어 있기 때문에 일반적인 전처리 방법으로는 학습에 한계가 있을 것으로 보입니다.
