{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"movie_pred.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPhKzHzCMduXMwoDjmnnm1M"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"4eEwHG_JtBjH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"3a612fb8-d05d-4776-c395-aba8586b2105","executionInfo":{"status":"ok","timestamp":1582826654106,"user_tz":-540,"elapsed":1395,"user":{"displayName":"‍김승유(학부학생/학부대학 공학계열)","photoUrl":"","userId":"01309828358390765454"}}},"source":["from google.colab import auth\n","auth.authenticate_user()\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=False)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6_fwy86ptHdJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"6fcdf254-a2a6-4114-8ef6-343df79b524a","executionInfo":{"status":"ok","timestamp":1582826654108,"user_tz":-540,"elapsed":1384,"user":{"displayName":"‍김승유(학부학생/학부대학 공학계열)","photoUrl":"","userId":"01309828358390765454"}}},"source":["import os\n","from pathlib import Path\n","\n","folder = \"colab/movie_pred_git\" ## 자기 드라이브 경로 입력\n","\n","base_path = Path(\"/content/gdrive/My Drive/\")\n","project_path = base_path / folder\n","os.chdir(project_path)\n","for x in list(project_path.glob(\"*\")):\n","    if x.is_dir():\n","        dir_name = str(x.relative_to(project_path))\n","        os.rename(dir_name, dir_name.split(\" \", 1)[0])\n","print(f\"{os.getcwd()}\")"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/colab/movie_pred_git\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Rpy7ZLV1_FOt","colab_type":"text"},"source":["# 패키지, 데이터 로딩, 하이퍼 파라미터 설정"]},{"cell_type":"code","metadata":{"id":"dpUzfq0-wCF6","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","from torch.utils.data import DataLoader, Dataset\n","import torch.nn.functional as F\n","import torch.nn as nn\n","import torch\n","import pickle\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PUYJxCj_wL7-","colab_type":"code","colab":{}},"source":["with open ('data/train.txt', 'rb') as f:\n","    train_data = pickle.load(f)\n","with open ('data/test.txt', 'rb') as f:\n","    test_data = pickle.load(f)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WNALuqkzwQdw","colab_type":"code","colab":{}},"source":["### 하이퍼 파라미터 세팅\n","batch_size = 100\n","learning_rate = 0.0005\n","num_epochs = 30\n","\n","# max_len = 50\n","embed_dim = 300\n","hid_dim = 300\n","\n","print_every = 250\n","example_every = 1000\n","save_every = 10"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CkwHSdHk_KFH","colab_type":"text"},"source":["# 딕셔너리, 데이터셋 및 bi-lstm 모델 구현"]},{"cell_type":"code","metadata":{"id":"AzjSEhgdwVKa","colab_type":"code","colab":{}},"source":["class Dictionary(object):\n","    \n","    def __init__(self, train_data):\n","        self.word2ix = {'<pad>': 0, '<UNK>':1} ## <pad>: 패딩, <UNK>: unknown\n","        self.ix2word = {0: '<pad>', 1:'<UNK>'}\n","        self.wordcnt = dict()      \n","        self.cnt = 1\n","        \n","        print('Making dict..')\n","        for i in train_data[\"document\"]:\n","            for word in i.split():\n","                if word not in self.word2ix:\n","                    self.word2ix[word] = self.cnt\n","                    self.ix2word[self.cnt] = word\n","                    self.wordcnt[word] = 0\n","                    self.cnt += 1\n","                else:\n","                    self.wordcnt[word] += 1  \n","        print(\"Complete\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4yfnyj2AwYzg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"399f1862-da62-4f74-99e2-afe533019445","executionInfo":{"status":"ok","timestamp":1582826655064,"user_tz":-540,"elapsed":2286,"user":{"displayName":"‍김승유(학부학생/학부대학 공학계열)","photoUrl":"","userId":"01309828358390765454"}}},"source":["train_dict = Dictionary(train_data)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Making dict..\n","Complete\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RbNkCPr8wdOx","colab_type":"code","colab":{}},"source":["class MovieData(Dataset):\n","    \n","    def __init__(self, data, dictionary, max_len=50):\n","        super(MovieData, self).__init__()\n","        self.dataset = data\n","        self.dictionary = dictionary\n","        self.max_len = max_len\n","        \n","    def __len__(self):\n","        return len(self.dataset)\n","    \n","    def __getitem__(self, ix):\n","        text = [self.dictionary.word2ix[word] if word in self.dictionary.word2ix.keys() else 1 for word in self.dataset[\"document\"][ix].split()]\n","        label = self.dataset[\"label\"][ix]\n","        text_len = len(text)\n","        if len(text) < self.max_len:\n","            text += [0 for i in range(self.max_len - len(text))]\n","        elif len(text) > self.max_len:\n","            text = text[:self.max_len]\n","            text_len = self.max_len    \n","        \n","        text = torch.LongTensor(text)\n","        return text, text_len, label"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9OG0cGvVwiSG","colab_type":"code","colab":{}},"source":["class Bi_LSTM(nn.Module):\n","    \n","    def __init__(self, num_text, embed_dim, hid_dim, fc_hid_dim = 300, max_len = 50,\n","                 dropout=0.1, num_layers=1, batch_first=True, bidirectional=True):\n","        \n","        super(Bi_LSTM, self).__init__()\n","        \n","\n","        self.text_embed = nn.Embedding(num_text, embed_dim)\n","        \n","        self.lstm = nn.LSTM(embed_dim, hid_dim, dropout=dropout, num_layers=num_layers, batch_first=batch_first, bidirectional=bidirectional)\n","\n","        self.fc_dim = hid_dim * 2 if bidirectional else hid_dim * 1\n","        self.fc = nn.Sequential(nn.Linear(self.fc_dim, fc_hid_dim), nn.Linear(fc_hid_dim, 2))\n","        \n","    def forward(self, text, text_len):\n","        text = self.text_embed(text)\n","\n","        text = nn.utils.rnn.pack_padded_sequence(text, text_len, \n","                                                batch_first=True, enforce_sorted=False)\n","        out, (hidden, cell) = self.lstm(text)\n","\n","        out, out_len = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n","        \n","        for i in range(len(text_len)):\n","          out[i][-1] = out[i][text_len[i]-1]\n","\n","        out = self.fc(out[:,-1,:])\n","\n","        out = torch.sigmoid(out)\n","        return out"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hycn0Idd_Btb","colab_type":"text"},"source":["# 학습"]},{"cell_type":"code","metadata":{"id":"JanIoLx3wkvx","colab_type":"code","colab":{}},"source":["train_dataset = MovieData(train_data, train_dict)\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n","                                           batch_size=batch_size, \n","                                           shuffle=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oAINT7wuwscS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"outputId":"215fefe5-a7f9-4f25-9c3f-976b18e453f2","executionInfo":{"status":"ok","timestamp":1582826659137,"user_tz":-540,"elapsed":6323,"user":{"displayName":"‍김승유(학부학생/학부대학 공학계열)","photoUrl":"","userId":"01309828358390765454"}}},"source":["model = Bi_LSTM(len(train_dict.word2ix), embed_dim, hid_dim).to(device)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"gCBMw6Rews6L","colab_type":"code","colab":{}},"source":["criterion = nn.CrossEntropyLoss().to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-KeoI1LCw0uq","colab_type":"code","colab":{}},"source":["def save_checkpoint(epoch, model, optimizer):\n","  torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict()\n","            }, \"model/model_epoch\"+str(epoch) + \".tar\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"u99puj4ow2E2","colab_type":"code","colab":{}},"source":["def train(num_epochs, model, data_loader, criterion, optimizer):\n","    print('Training..')\n","    for epoch in range(num_epochs):\n","        for i, (texts, text_lens, labels) in enumerate(data_loader):\n","\n","            texts, text_lens, labels = texts.to(device), text_lens.to(device), labels.to(device)\n","            outputs = model(texts, text_lens)\n","            loss = criterion(outputs, labels)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step() \n","\n","            ## accuracy 계산\n","            _, argmax = torch.max(outputs, 1)\n","            accuracy = (labels == argmax).float().mean()\n","\n","            if (i+1) % print_every == 0: ## 상태 출력\n","                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'.format(\n","                    epoch+1, num_epochs, i+1, len(data_loader), loss.item(), accuracy.item() * 100))\n","                \n","            if (i+1) % example_every == 0: ## csv 파일로 현재 모델 출력 표시\n","              if not os.path.exists(\"example\"):\n","                os.makedirs(\"example\")\n","              ex = pd.DataFrame()\n","              ex[\"label\"] = labels.to('cpu')\n","              ex[\"pred\"] = argmax.to('cpu')\n","              text_lens=text_lens.to('cpu')\n","\n","              tmp_text = []\n","              for j in range(batch_size):\n","                tmp_txt = \"\"\n","                for k in range(text_lens[j]):\n","                  tmp_txt += train_dict.ix2word[texts.tolist()[j][k]]+\" \"\n","                tmp_text.append(tmp_txt)\n","              ex[\"document\"] = tmp_text\n","              ex.to_csv(\"example/example_epoch\"+str(epoch+1)+\"_iter\"+str(i)+\".csv\", mode='w')\n","              print(\"example saved!\")\n","            \n","        ## checkpoint 저장\n","        if (epoch+1) % save_every == 0:\n","           if not os.path.exists(\"model\"):\n","             os.makedirs(\"model\")\n","           save_checkpoint(epoch+1, model, optimizer)\n","           print(\"checkpoint saved!\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XYgcwvIvxKnU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"6cafc4c9-c768-4b2d-868d-db27c65fc88b","executionInfo":{"status":"ok","timestamp":1582829247144,"user_tz":-540,"elapsed":678797,"user":{"displayName":"‍김승유(학부학생/학부대학 공학계열)","photoUrl":"","userId":"01309828358390765454"}}},"source":["train(num_epochs, model, train_loader, criterion, optimizer)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Training..\n","Epoch [1/30], Step [250/1487], Loss: 0.5328, Accuracy: 75.00%\n","Epoch [1/30], Step [500/1487], Loss: 0.5248, Accuracy: 77.00%\n","Epoch [1/30], Step [750/1487], Loss: 0.4827, Accuracy: 81.00%\n","Epoch [1/30], Step [1000/1487], Loss: 0.4690, Accuracy: 83.00%\n","example saved!\n","Epoch [1/30], Step [1250/1487], Loss: 0.5065, Accuracy: 78.00%\n","Epoch [2/30], Step [250/1487], Loss: 0.4292, Accuracy: 89.00%\n","Epoch [2/30], Step [500/1487], Loss: 0.3980, Accuracy: 91.00%\n","Epoch [2/30], Step [750/1487], Loss: 0.4976, Accuracy: 82.00%\n","Epoch [2/30], Step [1000/1487], Loss: 0.4168, Accuracy: 91.00%\n","example saved!\n","Epoch [2/30], Step [1250/1487], Loss: 0.4925, Accuracy: 81.00%\n","Epoch [3/30], Step [250/1487], Loss: 0.4363, Accuracy: 86.00%\n","Epoch [3/30], Step [500/1487], Loss: 0.4523, Accuracy: 85.00%\n","Epoch [3/30], Step [750/1487], Loss: 0.4113, Accuracy: 90.00%\n","Epoch [3/30], Step [1000/1487], Loss: 0.4155, Accuracy: 90.00%\n","example saved!\n","Epoch [3/30], Step [1250/1487], Loss: 0.3528, Accuracy: 96.00%\n","Epoch [4/30], Step [250/1487], Loss: 0.4146, Accuracy: 90.00%\n","Epoch [4/30], Step [500/1487], Loss: 0.4012, Accuracy: 90.00%\n","Epoch [4/30], Step [750/1487], Loss: 0.4036, Accuracy: 91.00%\n","Epoch [4/30], Step [1000/1487], Loss: 0.3789, Accuracy: 93.00%\n","example saved!\n","Epoch [4/30], Step [1250/1487], Loss: 0.3492, Accuracy: 96.00%\n","Epoch [5/30], Step [250/1487], Loss: 0.4157, Accuracy: 89.00%\n","Epoch [5/30], Step [500/1487], Loss: 0.4501, Accuracy: 87.00%\n","Epoch [5/30], Step [750/1487], Loss: 0.3492, Accuracy: 97.00%\n","Epoch [5/30], Step [1000/1487], Loss: 0.3835, Accuracy: 93.00%\n","example saved!\n","Epoch [5/30], Step [1250/1487], Loss: 0.3898, Accuracy: 92.00%\n","Epoch [6/30], Step [250/1487], Loss: 0.4134, Accuracy: 90.00%\n","Epoch [6/30], Step [500/1487], Loss: 0.3716, Accuracy: 94.00%\n","Epoch [6/30], Step [750/1487], Loss: 0.3893, Accuracy: 92.00%\n","Epoch [6/30], Step [1000/1487], Loss: 0.4659, Accuracy: 84.00%\n","example saved!\n","Epoch [6/30], Step [1250/1487], Loss: 0.3577, Accuracy: 96.00%\n","Epoch [7/30], Step [250/1487], Loss: 0.3857, Accuracy: 93.00%\n","Epoch [7/30], Step [500/1487], Loss: 0.3977, Accuracy: 91.00%\n","Epoch [7/30], Step [750/1487], Loss: 0.3790, Accuracy: 93.00%\n","Epoch [7/30], Step [1000/1487], Loss: 0.3666, Accuracy: 95.00%\n","example saved!\n","Epoch [7/30], Step [1250/1487], Loss: 0.3697, Accuracy: 94.00%\n","Epoch [8/30], Step [250/1487], Loss: 0.3744, Accuracy: 94.00%\n","Epoch [8/30], Step [500/1487], Loss: 0.3697, Accuracy: 94.00%\n","Epoch [8/30], Step [750/1487], Loss: 0.3883, Accuracy: 92.00%\n","Epoch [8/30], Step [1000/1487], Loss: 0.3624, Accuracy: 95.00%\n","example saved!\n","Epoch [8/30], Step [1250/1487], Loss: 0.3437, Accuracy: 97.00%\n","Epoch [9/30], Step [250/1487], Loss: 0.3447, Accuracy: 97.00%\n","Epoch [9/30], Step [500/1487], Loss: 0.3742, Accuracy: 94.00%\n","Epoch [9/30], Step [750/1487], Loss: 0.3538, Accuracy: 96.00%\n","Epoch [9/30], Step [1000/1487], Loss: 0.3956, Accuracy: 92.00%\n","example saved!\n","Epoch [9/30], Step [1250/1487], Loss: 0.3598, Accuracy: 95.00%\n","Epoch [10/30], Step [250/1487], Loss: 0.3633, Accuracy: 95.00%\n","Epoch [10/30], Step [500/1487], Loss: 0.3657, Accuracy: 94.00%\n","Epoch [10/30], Step [750/1487], Loss: 0.4018, Accuracy: 92.00%\n","Epoch [10/30], Step [1000/1487], Loss: 0.4231, Accuracy: 89.00%\n","example saved!\n","Epoch [10/30], Step [1250/1487], Loss: 0.3632, Accuracy: 95.00%\n","checkpoint saved!\n","Epoch [11/30], Step [250/1487], Loss: 0.3641, Accuracy: 95.00%\n","Epoch [11/30], Step [500/1487], Loss: 0.3759, Accuracy: 94.00%\n","Epoch [11/30], Step [750/1487], Loss: 0.3733, Accuracy: 94.00%\n","Epoch [11/30], Step [1000/1487], Loss: 0.3587, Accuracy: 95.00%\n","example saved!\n","Epoch [11/30], Step [1250/1487], Loss: 0.3764, Accuracy: 94.00%\n","Epoch [12/30], Step [250/1487], Loss: 0.4241, Accuracy: 89.00%\n","Epoch [12/30], Step [500/1487], Loss: 0.3704, Accuracy: 94.00%\n","Epoch [12/30], Step [750/1487], Loss: 0.3914, Accuracy: 92.00%\n","Epoch [12/30], Step [1000/1487], Loss: 0.3732, Accuracy: 94.00%\n","example saved!\n","Epoch [12/30], Step [1250/1487], Loss: 0.3606, Accuracy: 95.00%\n","Epoch [13/30], Step [250/1487], Loss: 0.4227, Accuracy: 89.00%\n","Epoch [13/30], Step [500/1487], Loss: 0.3620, Accuracy: 95.00%\n","Epoch [13/30], Step [750/1487], Loss: 0.3964, Accuracy: 91.00%\n","Epoch [13/30], Step [1000/1487], Loss: 0.3633, Accuracy: 95.00%\n","example saved!\n","Epoch [13/30], Step [1250/1487], Loss: 0.3334, Accuracy: 98.00%\n","Epoch [14/30], Step [250/1487], Loss: 0.3734, Accuracy: 94.00%\n","Epoch [14/30], Step [500/1487], Loss: 0.3533, Accuracy: 96.00%\n","Epoch [14/30], Step [750/1487], Loss: 0.3533, Accuracy: 96.00%\n","Epoch [14/30], Step [1000/1487], Loss: 0.4150, Accuracy: 90.00%\n","example saved!\n","Epoch [14/30], Step [1250/1487], Loss: 0.3432, Accuracy: 97.00%\n","Epoch [15/30], Step [250/1487], Loss: 0.3731, Accuracy: 94.00%\n","Epoch [15/30], Step [500/1487], Loss: 0.3333, Accuracy: 98.00%\n","Epoch [15/30], Step [750/1487], Loss: 0.3436, Accuracy: 97.00%\n","Epoch [15/30], Step [1000/1487], Loss: 0.4030, Accuracy: 91.00%\n","example saved!\n","Epoch [15/30], Step [1250/1487], Loss: 0.3337, Accuracy: 98.00%\n","Epoch [16/30], Step [250/1487], Loss: 0.3644, Accuracy: 95.00%\n","Epoch [16/30], Step [500/1487], Loss: 0.3818, Accuracy: 92.00%\n","Epoch [16/30], Step [750/1487], Loss: 0.3742, Accuracy: 94.00%\n","Epoch [16/30], Step [1000/1487], Loss: 0.3535, Accuracy: 96.00%\n","example saved!\n","Epoch [16/30], Step [1250/1487], Loss: 0.3933, Accuracy: 92.00%\n","Epoch [17/30], Step [250/1487], Loss: 0.3458, Accuracy: 97.00%\n","Epoch [17/30], Step [500/1487], Loss: 0.3376, Accuracy: 97.00%\n","Epoch [17/30], Step [750/1487], Loss: 0.3542, Accuracy: 96.00%\n","Epoch [17/30], Step [1000/1487], Loss: 0.3508, Accuracy: 97.00%\n","example saved!\n","Epoch [17/30], Step [1250/1487], Loss: 0.3233, Accuracy: 99.00%\n","Epoch [18/30], Step [250/1487], Loss: 0.3432, Accuracy: 97.00%\n","Epoch [18/30], Step [500/1487], Loss: 0.3633, Accuracy: 95.00%\n","Epoch [18/30], Step [750/1487], Loss: 0.3628, Accuracy: 95.00%\n","Epoch [18/30], Step [1000/1487], Loss: 0.3624, Accuracy: 95.00%\n","example saved!\n","Epoch [18/30], Step [1250/1487], Loss: 0.3841, Accuracy: 93.00%\n","Epoch [19/30], Step [250/1487], Loss: 0.3728, Accuracy: 94.00%\n","Epoch [19/30], Step [500/1487], Loss: 0.3438, Accuracy: 97.00%\n","Epoch [19/30], Step [750/1487], Loss: 0.3775, Accuracy: 93.00%\n","Epoch [19/30], Step [1000/1487], Loss: 0.3434, Accuracy: 97.00%\n","example saved!\n","Epoch [19/30], Step [1250/1487], Loss: 0.3911, Accuracy: 92.00%\n","Epoch [20/30], Step [250/1487], Loss: 0.3433, Accuracy: 97.00%\n","Epoch [20/30], Step [500/1487], Loss: 0.3435, Accuracy: 97.00%\n","Epoch [20/30], Step [750/1487], Loss: 0.3896, Accuracy: 92.00%\n","Epoch [20/30], Step [1000/1487], Loss: 0.3666, Accuracy: 95.00%\n","example saved!\n","Epoch [20/30], Step [1250/1487], Loss: 0.3433, Accuracy: 97.00%\n","checkpoint saved!\n","Epoch [21/30], Step [250/1487], Loss: 0.3533, Accuracy: 96.00%\n","Epoch [21/30], Step [500/1487], Loss: 0.3335, Accuracy: 98.00%\n","Epoch [21/30], Step [750/1487], Loss: 0.3733, Accuracy: 94.00%\n","Epoch [21/30], Step [1000/1487], Loss: 0.4069, Accuracy: 91.00%\n","example saved!\n","Epoch [21/30], Step [1250/1487], Loss: 0.3545, Accuracy: 96.00%\n","Epoch [22/30], Step [250/1487], Loss: 0.3933, Accuracy: 92.00%\n","Epoch [22/30], Step [500/1487], Loss: 0.3528, Accuracy: 96.00%\n","Epoch [22/30], Step [750/1487], Loss: 0.3719, Accuracy: 94.00%\n","Epoch [22/30], Step [1000/1487], Loss: 0.3742, Accuracy: 94.00%\n","example saved!\n","Epoch [22/30], Step [1250/1487], Loss: 0.3583, Accuracy: 95.00%\n","Epoch [23/30], Step [250/1487], Loss: 0.3833, Accuracy: 93.00%\n","Epoch [23/30], Step [500/1487], Loss: 0.3746, Accuracy: 94.00%\n","Epoch [23/30], Step [750/1487], Loss: 0.3555, Accuracy: 96.00%\n","Epoch [23/30], Step [1000/1487], Loss: 0.3451, Accuracy: 97.00%\n","example saved!\n","Epoch [23/30], Step [1250/1487], Loss: 0.3832, Accuracy: 93.00%\n","Epoch [24/30], Step [250/1487], Loss: 0.3534, Accuracy: 96.00%\n","Epoch [24/30], Step [500/1487], Loss: 0.3646, Accuracy: 95.00%\n","Epoch [24/30], Step [750/1487], Loss: 0.3861, Accuracy: 93.00%\n","Epoch [24/30], Step [1000/1487], Loss: 0.3733, Accuracy: 94.00%\n","example saved!\n","Epoch [24/30], Step [1250/1487], Loss: 0.4034, Accuracy: 91.00%\n","Epoch [25/30], Step [250/1487], Loss: 0.3533, Accuracy: 96.00%\n","Epoch [25/30], Step [500/1487], Loss: 0.3433, Accuracy: 97.00%\n","Epoch [25/30], Step [750/1487], Loss: 0.3933, Accuracy: 92.00%\n","Epoch [25/30], Step [1000/1487], Loss: 0.3533, Accuracy: 96.00%\n","example saved!\n","Epoch [25/30], Step [1250/1487], Loss: 0.3433, Accuracy: 97.00%\n","Epoch [26/30], Step [250/1487], Loss: 0.3633, Accuracy: 95.00%\n","Epoch [26/30], Step [500/1487], Loss: 0.3237, Accuracy: 99.00%\n","Epoch [26/30], Step [750/1487], Loss: 0.3632, Accuracy: 95.00%\n","Epoch [26/30], Step [1000/1487], Loss: 0.3534, Accuracy: 96.00%\n","example saved!\n","Epoch [26/30], Step [1250/1487], Loss: 0.3564, Accuracy: 96.00%\n","Epoch [27/30], Step [250/1487], Loss: 0.3611, Accuracy: 95.00%\n","Epoch [27/30], Step [500/1487], Loss: 0.3741, Accuracy: 94.00%\n","Epoch [27/30], Step [750/1487], Loss: 0.4333, Accuracy: 88.00%\n","Epoch [27/30], Step [1000/1487], Loss: 0.3635, Accuracy: 95.00%\n","example saved!\n","Epoch [27/30], Step [1250/1487], Loss: 0.3733, Accuracy: 94.00%\n","Epoch [28/30], Step [250/1487], Loss: 0.3535, Accuracy: 96.00%\n","Epoch [28/30], Step [500/1487], Loss: 0.3333, Accuracy: 98.00%\n","Epoch [28/30], Step [750/1487], Loss: 0.3667, Accuracy: 95.00%\n","Epoch [28/30], Step [1000/1487], Loss: 0.3699, Accuracy: 94.00%\n","example saved!\n","Epoch [28/30], Step [1250/1487], Loss: 0.3328, Accuracy: 98.00%\n","Epoch [29/30], Step [250/1487], Loss: 0.3812, Accuracy: 93.00%\n","Epoch [29/30], Step [500/1487], Loss: 0.3520, Accuracy: 96.00%\n","Epoch [29/30], Step [750/1487], Loss: 0.3333, Accuracy: 98.00%\n","Epoch [29/30], Step [1000/1487], Loss: 0.3420, Accuracy: 97.00%\n","example saved!\n","Epoch [29/30], Step [1250/1487], Loss: 0.4027, Accuracy: 91.00%\n","Epoch [30/30], Step [250/1487], Loss: 0.3533, Accuracy: 96.00%\n","Epoch [30/30], Step [500/1487], Loss: 0.3615, Accuracy: 95.00%\n","Epoch [30/30], Step [750/1487], Loss: 0.3833, Accuracy: 93.00%\n","Epoch [30/30], Step [1000/1487], Loss: 0.3943, Accuracy: 92.00%\n","example saved!\n","Epoch [30/30], Step [1250/1487], Loss: 0.3860, Accuracy: 93.00%\n","checkpoint saved!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6YeVQaV--4Px","colab_type":"text"},"source":["# 테스트"]},{"cell_type":"code","metadata":{"id":"5Q07wXDZxOga","colab_type":"code","colab":{}},"source":["def test(model, data_loader):\n","    print('Testing..')\n","    model.eval()\n","    with torch.no_grad():\n","        correct = 0\n","        total = 0\n","        for i, (texts, text_lens, labels) in enumerate(data_loader):\n","            texts, text_lens, labels = texts.to(device), text_lens.to(device), labels.to(device)\n","            outputs = model(texts, text_lens)\n","\n","            _, argmax = torch.max(outputs, 1)\n","            total += texts.size(0)\n","            correct += (labels == argmax).sum().item()\n","\n","        print('Test accuracy for {} texts: {:.2f}%'.format(total, correct / total * 100))\n","    model.train() ## 다시 돌려놓기"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dusQGCNTxZP2","colab_type":"code","colab":{}},"source":["test_dataset = MovieData(test_data, train_dict)\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n","                                           batch_size=batch_size, \n","                                           shuffle=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qHZ1N75Uxayy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":90},"outputId":"8460cf7a-a3a9-41fd-9c13-ffd5847213ab","executionInfo":{"status":"ok","timestamp":1582829247578,"user_tz":-540,"elapsed":12,"user":{"displayName":"‍김승유(학부학생/학부대학 공학계열)","photoUrl":"","userId":"01309828358390765454"}}},"source":["checkpoint = torch.load(\"model/model_epoch30.tar\")\n","model = Bi_LSTM(len(train_dict.word2ix), embed_dim, hid_dim).to(device)\n","model.load_state_dict(checkpoint['model_state_dict']) "],"execution_count":18,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"unJSK5okxb_1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"2f1ef748-4082-462a-a8b9-26803876bda9","executionInfo":{"status":"ok","timestamp":1582829255171,"user_tz":-540,"elapsed":7596,"user":{"displayName":"‍김승유(학부학생/학부대학 공학계열)","photoUrl":"","userId":"01309828358390765454"}}},"source":["test(model, test_loader)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Testing..\n","Test accuracy for 49518 texts: 84.61%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"66-Ci-K8xePl","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}